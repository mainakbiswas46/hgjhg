<h2>Distributed Key-Value Store</h2> <ul> <li>Multiple clients communicate with a single coordinating server (Master) using a JSON-based message format over TCP sockets.</li> <li><b>Fault Tolerance</b>: The system ensures data durability, meaning no data is lost even if a single node fails. This is achieved through a data replication factor of 2, implemented using consistent hashing.</li> <li><b>Atomic Operations (Using 2-Phase Commit Protocol)</b>: All operations on the store are atomic. Either the operation succeeds entirely or fails completely, ensuring no side effects.</li> </ul> <h2>System Architecture</h2> The system consists of three main entities: <ol type="A"> <li><b><i>Distributed Key-Value Store Clients</i></b> Clients register or log in to the system and interact with data using the following operations: <ol type="i"> <li>GET &ltkey&gt</li> <li>PUT &ltkey&gt &ltvalue&gt</li> <li>DELETE &ltkey&gt</li> <li>UPDATE &ltkey&gt &ltnew_value&gt</li> </ol> </li> <li><b><i>Coordination Server</i></b> <ol type="i"> <li>Acts as an intermediary between clients and slave servers.</li> <li>When a new slave server registers, the Coordination Server hashes the `ip:port` of the server and places it in a ring structure using consistent hashing.</li> <li>When a client sends a request, the key is hashed and mapped to the appropriate slave server. The hashed key is stored in: <ul> <li>The "own table" of the next nearest hashed slave server.</li> <li>The "prev table" of the predecessor of this slave server.</li> </ul> </li> <li><b>Write-Through Cache:</b> The Coordination Server includes a write-through LRU cache. Requests are served directly from the cache when possible. The slave servers are contacted only on a cache miss.</li> </ol> </li> <li><b><i>Slave Servers</i></b> Slave servers store the actual data (key-value pairs) in two tables: <ol type="i"> <li><b>OWN Table</b>: Stores the first copies of keys with hash values greater than the ID of its immediate predecessor and up to its own ID.</li> <li><b>PREV Table</b>: Stores the keys whose first copies are stored in the predecessor's OWN table.</li> </ol> </li> </ol> <h2>Data Migration</h2> To maintain redundancy, each key is replicated on two slave servers. When a server joins or leaves the system, data migration ensures the required replication is maintained.
<b><i>Case 1: When a new Server registers</i></b>:

<ol> <li>The predecessor, successor, and successor of the successor are identified using the hash of the new server's ID.</li> <li>The successor updates its OWN table by removing keys hashed between the predecessor and the new server. These keys are: <ul> <li>Added to the PREV table of the successor.</li> <li>Stored in the OWN table of the new server.</li> </ul> </li> <li>The new server updates its PREV table with the OWN table of the predecessor.</li> <li>The successor of the successor updates its PREV table with the updated OWN table of the successor.</li> </ol>
<b><i>Case 2: When a Server goes down</i></b>:

<ol type="i"> <li>The successor, predecessor, and successor of the successor are identified using the hash of the dead server's ID.</li> <li>The successor (leader) of the dead server copies the content of its PREV table to its OWN table.</li> <li>The successor then sends the updated OWN table values to the successor of the successor of the dead server.</li> </ol> <h2>Heartbeat Implementation</h2> The heartbeat mechanism ensures that the system detects when a slave server goes down. Slave servers periodically send heartbeat messages to the Coordination Server (CS) to indicate they are alive. <ul> <li>A map at the CS tracks each slave server (`ip:port`) and increments a counter for every received heartbeat.</li> <li>A dedicated heartbeat thread on the slave server sends a UDP message to the CS every 5 seconds.</li> <li>The CS runs a timer thread that wakes up every 30 seconds to check the map. If a counter is found at 0, the CS identifies the corresponding slave server as down and initiates data migration.</li> </ul> <h2>Compile and Run Program</h2> <ol> <li>Compile `Coordination_Server.cpp` (use `-lpthreads` to support pthreads) and run it as: <br> `./coord &ltip&gt &ltport&gt`</li> <li>Register Slave Servers at the CS by running `slaveServer.cpp` as: <br> `./slave &ltip&gt &ltport&gt`</li> <li>Connect multiple clients to the Coordination Server by running `client.cpp` as: <br> `./client &ltip&gt &ltport&gt`</li> </ol>
A file cs_config.txt contains the IP and port of the Coordination Server.